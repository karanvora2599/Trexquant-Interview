{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9c9c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./final_output.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-running the single-cell notebook solution (session was reset).\n",
    "\n",
    "import os, re, glob, csv, html\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_DIR = './Training_Filings/'\n",
    "OUTPUT_CSV = './final_output.csv'\n",
    "\n",
    "def load_html_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw = html.unescape(f.read())\n",
    "    soup = BeautifulSoup(raw, \"html.parser\")\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\(\\s*(\\d+(?:\\.\\d+)?)\\s*\\)\", r\"-\\1\", text)  # bracket negatives\n",
    "    return text\n",
    "\n",
    "NUMBER = r\"(-?\\d+(?:\\.\\d+)?)\"\n",
    "PATTERNS = {\n",
    "    \"basic\": re.compile(rf\"(basic(?:\\s+and\\s+diluted)?\\s+(?:earnings|loss)\\s+per\\s+share.*?)({NUMBER})\", re.I),\n",
    "    \"diluted\": re.compile(rf\"(diluted\\s+(?:earnings|loss)\\s+per\\s+share.*?){NUMBER}\", re.I),\n",
    "    \"per_basic\": re.compile(rf\"({NUMBER})\\s+per\\s+basic\\s+share\", re.I),\n",
    "    \"per_diluted\": re.compile(rf\"({NUMBER})\\s+per\\s+diluted\\s+share\", re.I),\n",
    "    \"generic\": re.compile(rf\"(?:earnings|loss).{{0,80}}?\\s({NUMBER})\\s+per\\s+share\", re.I),\n",
    "}\n",
    "PREF_MAP = {\"basic\": 3, \"basic_and_diluted\": 2, \"diluted\": 1}\n",
    "\n",
    "def score_candidate(val: float, context: str):\n",
    "    score = 0.0\n",
    "    label = \"unknown\"\n",
    "    c = context.lower()\n",
    "    if \"basic\" in c and \"diluted\" not in c:\n",
    "        score += 5; label = \"basic\"\n",
    "    if \"diluted\" in c:\n",
    "        score += 3; label = \"diluted\"\n",
    "    if \"basic and diluted\" in c or \"diluted and basic\" in c:\n",
    "        score += 4; label = \"basic_and_diluted\"\n",
    "    if \"adjusted\" in c or \"non-gaap\" in c or \"non gaap\" in c:\n",
    "        score -= 3\n",
    "    if \"gaap\" in c:\n",
    "        score += 1\n",
    "    if any(q in c for q in [\"first quarter\",\"second quarter\",\"third quarter\",\"fourth quarter\",\"quarter\",\"q1\",\"q2\",\"q3\",\"q4\"]):\n",
    "        score += 1\n",
    "    if \"loss\" in c and val > 0:\n",
    "        val = -val\n",
    "    return score, val, label\n",
    "\n",
    "def extract_eps_from_text(text: str):\n",
    "    cands = []\n",
    "    for tag, patt in PATTERNS.items():\n",
    "        for m in patt.finditer(text):\n",
    "            ctx = text[max(0, m.start()-160): m.end()+160]\n",
    "            if re.search(r\"\\b\\d+\\s*pt\\b\", ctx, re.I):\n",
    "                continue\n",
    "            if tag in (\"generic\",\"per_basic\",\"per_diluted\"):\n",
    "                cl = ctx.lower()\n",
    "                if \"earnings\" not in cl and \"loss\" not in cl:\n",
    "                    continue\n",
    "            val = float(m.group(2) if tag in (\"basic\",\"diluted\") else m.group(1))\n",
    "            if abs(val) > 10 and \"loss\" not in ctx.lower():\n",
    "                continue\n",
    "            score, adj, lbl = score_candidate(val, ctx)\n",
    "            cands.append((score, adj, lbl if lbl!=\"unknown\" else tag, ctx))\n",
    "    if not cands:\n",
    "        return None, None\n",
    "    cands.sort(key=lambda x: (x[0], PREF_MAP.get(x[2], 0)), reverse=True)\n",
    "    best = cands[0]\n",
    "    return round(best[1], 2), best[2]\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(INPUT_DIR, \"0000*.html\")))\n",
    "rows = []\n",
    "for path in files:\n",
    "    fname = os.path.basename(path)\n",
    "    try:\n",
    "        txt = load_html_text(path)\n",
    "        eps, note = extract_eps_from_text(txt)\n",
    "        rows.append({\"filename\": fname, \"EPS\": \"\" if eps is None else f\"{eps:.2f}\", \"note\": note or \"\"})\n",
    "    except Exception as e:\n",
    "        rows.append({\"filename\": fname, \"EPS\": \"\", \"note\": f\"error: {e}\"})\n",
    "\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"filename\",\"EPS\",\"note\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "df = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "OUTPUT_CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f04174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 50 files under c:\\Users\\karan\\Documents\\Projects\\Trexquant\\Training_Filings\n",
      "Wrote: c:\\Users\\karan\\Documents\\Projects\\Trexquant\\final_output.csv  |  Rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "EPS",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "note",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e7fea62a-b508-4fde-93ad-5521aba50b73",
       "rows": [
        [
         "0",
         "0000004977-20-000054.html",
         "0.78",
         "diluted"
        ],
        [
         "1",
         "0000008947-20-000044.html",
         "-0.34",
         "diluted"
        ],
        [
         "2",
         "0000046080-20-000050.html",
         "-0.51",
         "diluted"
        ],
        [
         "3",
         "0000066570-20-000013.html",
         "1.11",
         "diluted"
        ],
        [
         "4",
         "0000314808-20-000062.html",
         "",
         ""
        ],
        [
         "5",
         "0000706129-20-000012.html",
         "-8.00",
         "diluted"
        ],
        [
         "6",
         "0000846617-20-000024.html",
         "-0.47",
         "diluted"
        ],
        [
         "7",
         "0000874766-20-000033.html",
         "0.74",
         "diluted"
        ],
        [
         "8",
         "0000875320-20-000014.html",
         "",
         ""
        ],
        [
         "9",
         "0000892537-20-000010.html",
         "0.71",
         "diluted"
        ],
        [
         "10",
         "0000895419-20-000042.html",
         "-0.57",
         "basic_and_diluted"
        ],
        [
         "11",
         "0000939057-20-000186.html",
         "",
         ""
        ],
        [
         "12",
         "0000950103-20-008424.html",
         "-0.26",
         "basic"
        ],
        [
         "13",
         "0001008654-20-000048.html",
         "",
         ""
        ],
        [
         "14",
         "0001104659-20-052683.html",
         "",
         ""
        ],
        [
         "15",
         "0001104659-20-052792.html",
         "-2020.00",
         "diluted"
        ],
        [
         "16",
         "0001104659-20-053353.html",
         "",
         ""
        ],
        [
         "17",
         "0001104659-20-053534.html",
         "",
         ""
        ],
        [
         "18",
         "0001104659-20-053563.html",
         "-1.19",
         "diluted"
        ],
        [
         "19",
         "0001140361-20-010070.html",
         "0.42",
         "diluted"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>EPS</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000004977-20-000054.html</td>\n",
       "      <td>0.78</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000008947-20-000044.html</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000046080-20-000050.html</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000066570-20-000013.html</td>\n",
       "      <td>1.11</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000314808-20-000062.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0000706129-20-000012.html</td>\n",
       "      <td>-8.00</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0000846617-20-000024.html</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0000874766-20-000033.html</td>\n",
       "      <td>0.74</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0000875320-20-000014.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0000892537-20-000010.html</td>\n",
       "      <td>0.71</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0000895419-20-000042.html</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>basic_and_diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0000939057-20-000186.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0000950103-20-008424.html</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0001008654-20-000048.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0001104659-20-052683.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0001104659-20-052792.html</td>\n",
       "      <td>-2020.00</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0001104659-20-053353.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0001104659-20-053534.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0001104659-20-053563.html</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0001140361-20-010070.html</td>\n",
       "      <td>0.42</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename       EPS               note\n",
       "0   0000004977-20-000054.html      0.78            diluted\n",
       "1   0000008947-20-000044.html     -0.34            diluted\n",
       "2   0000046080-20-000050.html     -0.51            diluted\n",
       "3   0000066570-20-000013.html      1.11            diluted\n",
       "4   0000314808-20-000062.html                             \n",
       "5   0000706129-20-000012.html     -8.00            diluted\n",
       "6   0000846617-20-000024.html     -0.47            diluted\n",
       "7   0000874766-20-000033.html      0.74            diluted\n",
       "8   0000875320-20-000014.html                             \n",
       "9   0000892537-20-000010.html      0.71            diluted\n",
       "10  0000895419-20-000042.html     -0.57  basic_and_diluted\n",
       "11  0000939057-20-000186.html                             \n",
       "12  0000950103-20-008424.html     -0.26              basic\n",
       "13  0001008654-20-000048.html                             \n",
       "14  0001104659-20-052683.html                             \n",
       "15  0001104659-20-052792.html  -2020.00            diluted\n",
       "16  0001104659-20-053353.html                             \n",
       "17  0001104659-20-053534.html                             \n",
       "18  0001104659-20-053563.html     -1.19            diluted\n",
       "19  0001140361-20-010070.html      0.42            diluted"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Single-cell EDGAR EPS parser (recursive, case-insensitive file discovery)\n",
    "\n",
    "# If BeautifulSoup isn't installed:\n",
    "# !pip install beautifulsoup4 lxml\n",
    "\n",
    "import os, re, glob, csv, html, sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# -------- Settings --------\n",
    "# Change this to your folder with the filings:\n",
    "INPUT_DIR = './Training_Filings/'\n",
    "OUTPUT_CSV = './final_output.csv'\n",
    "\n",
    "# -------- Discover files (recursive; case-insensitive .htm/.html) --------\n",
    "patterns = [\"**/*.html\", \"**/*.htm\", \"**/*.HTML\", \"**/*.HTM\"]\n",
    "files = []\n",
    "for p in patterns:\n",
    "    files.extend(glob.glob(os.path.join(INPUT_DIR, p), recursive=True))\n",
    "# De-duplicate & sort\n",
    "files = sorted(set(files))\n",
    "\n",
    "print(f\"Discovered {len(files)} files under {os.path.abspath(INPUT_DIR)}\")\n",
    "if len(files) == 0:\n",
    "    raise FileNotFoundError(\"No .html/.htm files found. Check INPUT_DIR.\")\n",
    "\n",
    "# -------- Helpers --------\n",
    "def load_html_text(path: str) -> str:\n",
    "    \"\"\"Load HTML, strip tags to text, normalize whitespace, convert bracketed negatives.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw = html.unescape(f.read())\n",
    "    # lxml is robust; if not present, switch to 'html.parser'\n",
    "    soup = BeautifulSoup(raw, \"lxml\")\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # (1.23) -> -1.23\n",
    "    text = re.sub(r\"\\(\\s*(\\d+(?:\\.\\d+)?)\\s*\\)\", r\"-\\1\", text)\n",
    "    return text\n",
    "\n",
    "NUMBER = r\"(-?\\d+(?:\\.\\d+)?)\"\n",
    "PATTERNS = {\n",
    "    \"basic\": re.compile(rf\"(basic(?:\\s+and\\s+diluted)?\\s+(?:earnings|loss)\\s+per\\s+share.*?)({NUMBER})\", re.I),\n",
    "    \"diluted\": re.compile(rf\"(diluted\\s+(?:earnings|loss)\\s+per\\s+share.*?){NUMBER}\", re.I),\n",
    "    \"per_basic\": re.compile(rf\"({NUMBER})\\s+per\\s+basic\\s+share\", re.I),\n",
    "    \"per_diluted\": re.compile(rf\"({NUMBER})\\s+per\\s+diluted\\s+share\", re.I),\n",
    "    # Allow up to 80 chars between \"earnings\"/\"loss\" and \"per share\"\n",
    "    \"generic\": re.compile(rf\"(?:earnings|loss).{{0,80}}?\\s({NUMBER})\\s+per\\s+share\", re.I),\n",
    "}\n",
    "\n",
    "# tie-breaker: prefer basic over diluted when scores tie\n",
    "PREF_MAP = {\"basic\": 3, \"basic_and_diluted\": 2, \"diluted\": 1}\n",
    "\n",
    "def score_candidate(val: float, context: str):\n",
    "    \"\"\"Assign a score and canonical label based on GAAP-ish signals.\"\"\"\n",
    "    score = 0.0\n",
    "    label = \"unknown\"\n",
    "    c = context.lower()\n",
    "\n",
    "    # Preference: basic > (basic and diluted) > diluted\n",
    "    if \"basic\" in c and \"diluted\" not in c:\n",
    "        score += 5; label = \"basic\"\n",
    "    if \"diluted\" in c:\n",
    "        score += 3; label = \"diluted\"\n",
    "    if \"basic and diluted\" in c or \"diluted and basic\" in c:\n",
    "        score += 4; label = \"basic_and_diluted\"\n",
    "\n",
    "    # Demote adjusted / non-GAAP\n",
    "    if \"adjusted\" in c or \"non-gaap\" in c or \"non gaap\" in c:\n",
    "        score -= 3\n",
    "\n",
    "    # Slight bump for explicit GAAP mentions\n",
    "    if \"gaap\" in c:\n",
    "        score += 1\n",
    "\n",
    "    # Small bump for quarterly language (recency / section relevance)\n",
    "    if any(q in c for q in [\"first quarter\",\"second quarter\",\"third quarter\",\"fourth quarter\",\"quarter\",\"q1\",\"q2\",\"q3\",\"q4\"]):\n",
    "        score += 1\n",
    "\n",
    "    # If explicitly a loss and value is positive, flip sign\n",
    "    if \"loss\" in c and val > 0:\n",
    "        val = -val\n",
    "\n",
    "    return score, val, label\n",
    "\n",
    "def extract_eps_from_text(text: str):\n",
    "    \"\"\"Return best (eps_value, note_label) or (None, None).\"\"\"\n",
    "    cands = []\n",
    "\n",
    "    for tag, patt in PATTERNS.items():\n",
    "        for m in patt.finditer(text):\n",
    "            # Context window helps identify GAAP vs adjusted, quarter, loss/earnings, etc.\n",
    "            ctx = text[max(0, m.start()-160): m.end()+160]\n",
    "\n",
    "            # Filter layout noise like \"12pt\"\n",
    "            if re.search(r\"\\b\\d+\\s*pt\\b\", ctx, re.I):\n",
    "                continue\n",
    "\n",
    "            # Require 'earnings' or 'loss' for generic/per_* variants\n",
    "            if tag in (\"generic\",\"per_basic\",\"per_diluted\"):\n",
    "                cl = ctx.lower()\n",
    "                if \"earnings\" not in cl and \"loss\" not in cl:\n",
    "                    continue\n",
    "\n",
    "            # Extract number\n",
    "            val = float(m.group(2) if tag in (\"basic\",\"diluted\") else m.group(1))\n",
    "\n",
    "            # Exclude very large magnitudes unless \"loss\" is nearby (e.g., big GAAP loss per share)\n",
    "            if abs(val) > 10 and \"loss\" not in ctx.lower():\n",
    "                continue\n",
    "\n",
    "            score, adj, lbl = score_candidate(val, ctx)\n",
    "            cands.append((score, adj, lbl if lbl!=\"unknown\" else tag, ctx))\n",
    "\n",
    "    if not cands:\n",
    "        return None, None\n",
    "\n",
    "    # Pick the highest score; on ties prefer basic > basic_and_diluted > diluted\n",
    "    cands.sort(key=lambda x: (x[0], PREF_MAP.get(x[2], 0)), reverse=True)\n",
    "    best = cands[0]\n",
    "    return round(best[1], 2), best[2]\n",
    "\n",
    "# -------- Run on all discovered files --------\n",
    "rows = []\n",
    "for path in files:\n",
    "    fname = os.path.basename(path)\n",
    "    try:\n",
    "        txt = load_html_text(path)\n",
    "        eps, note = extract_eps_from_text(txt)\n",
    "        rows.append({\"filename\": fname, \"EPS\": \"\" if eps is None else f\"{eps:.2f}\", \"note\": note or \"\"})\n",
    "    except Exception as e:\n",
    "        rows.append({\"filename\": fname, \"EPS\": \"\", \"note\": f\"error: {e}\"})\n",
    "\n",
    "# Save CSV and show a preview\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Wrote: {os.path.abspath(OUTPUT_CSV)}  |  Rows: {len(df)}\")\n",
    "display(df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad7e30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 50 files under c:\\Users\\karan\\Documents\\Projects\\Trexquant\\Training_Filings\n",
      "Wrote: c:\\Users\\karan\\Documents\\Projects\\Trexquant\\final_output.csv | Rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "eps",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "note",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "189728f7-12fc-468e-8843-2a2b968315ae",
       "rows": [
        [
         "0",
         "0000004977-20-000054.html",
         "0.78",
         "diluted"
        ],
        [
         "1",
         "0000008947-20-000044.html",
         "0.34",
         "diluted"
        ],
        [
         "2",
         "0000046080-20-000050.html",
         "-0.51",
         "diluted"
        ],
        [
         "3",
         "0000066570-20-000013.html",
         "1.11",
         "diluted"
        ],
        [
         "4",
         "0000314808-20-000062.html",
         "",
         ""
        ],
        [
         "5",
         "0000706129-20-000012.html",
         "-8.00",
         "diluted"
        ],
        [
         "6",
         "0000846617-20-000024.html",
         "0.47",
         "diluted"
        ],
        [
         "7",
         "0000874766-20-000033.html",
         "0.74",
         "diluted"
        ],
        [
         "8",
         "0000875320-20-000014.html",
         "",
         ""
        ],
        [
         "9",
         "0000892537-20-000010.html",
         "0.71",
         "diluted"
        ],
        [
         "10",
         "0000895419-20-000042.html",
         "-0.57",
         "diluted"
        ],
        [
         "11",
         "0000939057-20-000186.html",
         "",
         ""
        ],
        [
         "12",
         "0000950103-20-008424.html",
         "-0.26",
         "basic_and_diluted"
        ],
        [
         "13",
         "0001008654-20-000048.html",
         "",
         ""
        ],
        [
         "14",
         "0001104659-20-052683.html",
         "",
         ""
        ],
        [
         "15",
         "0001104659-20-052792.html",
         "-2020.00",
         "diluted"
        ],
        [
         "16",
         "0001104659-20-053353.html",
         "",
         ""
        ],
        [
         "17",
         "0001104659-20-053534.html",
         "",
         ""
        ],
        [
         "18",
         "0001104659-20-053563.html",
         "-1.19",
         "diluted"
        ],
        [
         "19",
         "0001140361-20-010070.html",
         "0.42",
         "diluted"
        ],
        [
         "20",
         "0001141391-20-000089.html",
         "1.68",
         "diluted"
        ],
        [
         "21",
         "0001157523-20-000597.html",
         "",
         ""
        ],
        [
         "22",
         "0001157523-20-000599.html",
         "0.56",
         "diluted"
        ],
        [
         "23",
         "0001157523-20-000600.html",
         "",
         ""
        ],
        [
         "24",
         "0001165002-20-000083.html",
         "-2020.00",
         "diluted"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 25
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>eps</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000004977-20-000054.html</td>\n",
       "      <td>0.78</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000008947-20-000044.html</td>\n",
       "      <td>0.34</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000046080-20-000050.html</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000066570-20-000013.html</td>\n",
       "      <td>1.11</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000314808-20-000062.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0000706129-20-000012.html</td>\n",
       "      <td>-8.00</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0000846617-20-000024.html</td>\n",
       "      <td>0.47</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0000874766-20-000033.html</td>\n",
       "      <td>0.74</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0000875320-20-000014.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0000892537-20-000010.html</td>\n",
       "      <td>0.71</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0000895419-20-000042.html</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0000939057-20-000186.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0000950103-20-008424.html</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>basic_and_diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0001008654-20-000048.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0001104659-20-052683.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0001104659-20-052792.html</td>\n",
       "      <td>-2020.00</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0001104659-20-053353.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0001104659-20-053534.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0001104659-20-053563.html</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0001140361-20-010070.html</td>\n",
       "      <td>0.42</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0001141391-20-000089.html</td>\n",
       "      <td>1.68</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0001157523-20-000597.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0001157523-20-000599.html</td>\n",
       "      <td>0.56</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0001157523-20-000600.html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0001165002-20-000083.html</td>\n",
       "      <td>-2020.00</td>\n",
       "      <td>diluted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename       eps               note\n",
       "0   0000004977-20-000054.html      0.78            diluted\n",
       "1   0000008947-20-000044.html      0.34            diluted\n",
       "2   0000046080-20-000050.html     -0.51            diluted\n",
       "3   0000066570-20-000013.html      1.11            diluted\n",
       "4   0000314808-20-000062.html                             \n",
       "5   0000706129-20-000012.html     -8.00            diluted\n",
       "6   0000846617-20-000024.html      0.47            diluted\n",
       "7   0000874766-20-000033.html      0.74            diluted\n",
       "8   0000875320-20-000014.html                             \n",
       "9   0000892537-20-000010.html      0.71            diluted\n",
       "10  0000895419-20-000042.html     -0.57            diluted\n",
       "11  0000939057-20-000186.html                             \n",
       "12  0000950103-20-008424.html     -0.26  basic_and_diluted\n",
       "13  0001008654-20-000048.html                             \n",
       "14  0001104659-20-052683.html                             \n",
       "15  0001104659-20-052792.html  -2020.00            diluted\n",
       "16  0001104659-20-053353.html                             \n",
       "17  0001104659-20-053534.html                             \n",
       "18  0001104659-20-053563.html     -1.19            diluted\n",
       "19  0001140361-20-010070.html      0.42            diluted\n",
       "20  0001141391-20-000089.html      1.68            diluted\n",
       "21  0001157523-20-000597.html                             \n",
       "22  0001157523-20-000599.html      0.56            diluted\n",
       "23  0001157523-20-000600.html                             \n",
       "24  0001165002-20-000083.html  -2020.00            diluted"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Single-cell EDGAR EPS parser (fixed)\n",
    "# - Recursive file discovery (.html/.htm, any case)\n",
    "# - Quarter-targeted extraction (prefers Q EPS over full-year lines)\n",
    "# - Strong preference for BASIC over DILUTED\n",
    "# - Demotes adjusted / non-GAAP\n",
    "# - Outputs: eps_notebook_output_v2.csv\n",
    "\n",
    "# If BeautifulSoup isn't installed:\n",
    "# !pip install beautifulsoup4 lxml\n",
    "\n",
    "import os, re, glob, csv, html\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# ====== SETTINGS ======\n",
    "INPUT_DIR = './Training_Filings/'\n",
    "OUTPUT_CSV = './final_output.csv'\n",
    "\n",
    "# ====== DISCOVER FILES (recursive; case-insensitive) ======\n",
    "patterns = [\"**/*.html\", \"**/*.htm\", \"**/*.HTML\", \"**/*.HTM\"]\n",
    "files = sorted(set(sum([glob.glob(os.path.join(INPUT_DIR, p), recursive=True) for p in patterns], [])))\n",
    "print(f\"Discovered {len(files)} files under {os.path.abspath(INPUT_DIR)}\")\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No .html/.htm files found. Double-check INPUT_DIR.\")\n",
    "\n",
    "# ====== HELPERS ======\n",
    "def load_html_text(path: str) -> str:\n",
    "    \"\"\"Load HTML, strip tags to text, normalize whitespace, convert bracketed negatives like (0.41)->-0.41.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw = html.unescape(f.read())\n",
    "    soup = BeautifulSoup(raw, \"lxml\")   # robust; if unavailable, switch to \"html.parser\"\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\(\\s*(\\d+(?:\\.\\d+)?)\\s*\\)\", r\"-\\1\", text)\n",
    "    return text\n",
    "\n",
    "NUMBER = r\"(-?\\d+(?:\\.\\d+)?)\"\n",
    "\n",
    "# Quarter phrases we want to anchor on\n",
    "QUARTER_BLOCK = re.compile(r\"((?:first|second|third|fourth)\\s+quarter.*?)\", re.I)\n",
    "\n",
    "# EPS patterns (general)\n",
    "PATTERNS = {\n",
    "    \"basic\": re.compile(rf\"(basic(?:\\s+and\\s+diluted)?\\s+(?:earnings|loss)\\s+per\\s+share.*?)({NUMBER})\", re.I),\n",
    "    \"diluted\": re.compile(rf\"(diluted\\s+(?:earnings|loss)\\s+per\\s+share.*?){NUMBER}\", re.I),\n",
    "    \"per_basic\": re.compile(rf\"({NUMBER})\\s+per\\s+basic\\s+share\", re.I),\n",
    "    \"per_diluted\": re.compile(rf\"({NUMBER})\\s+per\\s+diluted\\s+share\", re.I),\n",
    "    \"generic\": re.compile(rf\"(?:earnings|loss).{{0,120}}?\\s({NUMBER})\\s+per\\s+share\", re.I),\n",
    "}\n",
    "\n",
    "# tie-breaker weight: prefer basic > (basic_and_diluted) > diluted\n",
    "PREF_MAP = {\"basic\": 6, \"basic_and_diluted\": 3, \"diluted\": 1}\n",
    "\n",
    "def score_candidate(val: float, context: str, base_label: str):\n",
    "    \"\"\"Score a candidate number based on GAAP-ish signals; return (score, adjusted_val, label).\"\"\"\n",
    "    score = 0.0\n",
    "    label = base_label\n",
    "    c = context.lower()\n",
    "\n",
    "    # Strong preference for BASIC over DILUTED\n",
    "    if \"basic\" in c and \"diluted\" not in c:\n",
    "        score += 7; label = \"basic\"\n",
    "    if \"diluted\" in c:\n",
    "        score += 2; label = \"diluted\"\n",
    "    if \"basic and diluted\" in c or \"diluted and basic\" in c:\n",
    "        score += 5; label = \"basic_and_diluted\"\n",
    "\n",
    "    # Demote adjusted / non-GAAP\n",
    "    if \"adjusted\" in c or \"non-gaap\" in c or \"non gaap\" in c:\n",
    "        score -= 5\n",
    "\n",
    "    # Small bump for explicit GAAP and for quarter language\n",
    "    if \"gaap\" in c:\n",
    "        score += 1\n",
    "    if any(q in c for q in [\"first quarter\",\"second quarter\",\"third quarter\",\"fourth quarter\",\"quarter\",\"q1\",\"q2\",\"q3\",\"q4\"]):\n",
    "        score += 2\n",
    "\n",
    "    # Flip sign if explicitly a loss and value is positive\n",
    "    if \"loss\" in c and val > 0:\n",
    "        val = -val\n",
    "\n",
    "    return score, val, label\n",
    "\n",
    "def nearest_number_to_phrase(text, phrase_span, window=220):\n",
    "    \"\"\"Find the EPS-like number closest to a quarter phrase.\"\"\"\n",
    "    start, end = phrase_span\n",
    "    ctx = text[max(0, start-window): min(len(text), end+window)]\n",
    "    # Prefer '... per (diluted|basic) share' immediately near the quarter phrase\n",
    "    m = re.search(rf\"{NUMBER}\\s+per\\s+(?:diluted|basic)\\s+share\", ctx, re.I)\n",
    "    if m:\n",
    "        return float(m.group(1)), ctx\n",
    "    # Otherwise accept 'earnings/loss ... per share'\n",
    "    m = re.search(rf\"(?:earnings|loss).{{0,80}}?\\s({NUMBER})\\s+per\\s+share\", ctx, re.I)\n",
    "    if m:\n",
    "        return float(m.group(1)), ctx\n",
    "    return None, ctx\n",
    "\n",
    "def extract_eps_from_text(text: str):\n",
    "    cands = []\n",
    "\n",
    "    # 1) Quarter-targeted pass (preferred when present)\n",
    "    for qb in QUARTER_BLOCK.finditer(text):\n",
    "        val, ctx = nearest_number_to_phrase(text, qb.span(), window=220)\n",
    "        if val is not None:\n",
    "            # Allow big magnitudes only if 'loss' nearby (e.g., large GAAP loss per share)\n",
    "            if abs(val) > 10 and \"loss\" not in ctx.lower():\n",
    "                continue\n",
    "            score, adj, lbl = score_candidate(val, ctx, base_label=\"quarter\")\n",
    "            score += 4  # extra boost for quarter-anchored EPS\n",
    "            cands.append((score, adj, lbl, ctx))\n",
    "\n",
    "    # 2) General patterns\n",
    "    for tag, patt in PATTERNS.items():\n",
    "        for m in patt.finditer(text):\n",
    "            ctx = text[max(0, m.start()-180): m.end()+180]\n",
    "\n",
    "            # Filter formatting noise like \"12pt\"\n",
    "            if re.search(r\"\\b\\d+\\s*pt\\b\", ctx, re.I):\n",
    "                continue\n",
    "\n",
    "            # Require 'earnings' or 'loss' context for generic / per_* variants\n",
    "            if tag in (\"generic\",\"per_basic\",\"per_diluted\"):\n",
    "                cl = ctx.lower()\n",
    "                if \"earnings\" not in cl and \"loss\" not in cl:\n",
    "                    continue\n",
    "\n",
    "            # Extract the numeric value\n",
    "            val = float(m.group(2) if tag in (\"basic\",\"diluted\") else m.group(1))\n",
    "\n",
    "            # Exclude huge magnitudes unless an explicit loss context exists\n",
    "            if abs(val) > 10 and \"loss\" not in ctx.lower():\n",
    "                continue\n",
    "\n",
    "            score, adj, lbl = score_candidate(val, ctx, base_label=tag)\n",
    "            cands.append((score, adj, lbl, ctx))\n",
    "\n",
    "    if not cands:\n",
    "        return None, None\n",
    "\n",
    "    # Pick highest scoring; on ties, prefer basic > basic_and_diluted > diluted\n",
    "    cands.sort(key=lambda x: (x[0], PREF_MAP.get(x[2], 0)), reverse=True)\n",
    "    best = cands[0]\n",
    "    return round(best[1], 2), best[2]\n",
    "\n",
    "# ====== RUN ======\n",
    "rows = []\n",
    "for path in files:\n",
    "    fname = os.path.basename(path)\n",
    "    try:\n",
    "        txt = load_html_text(path)\n",
    "        eps, note = extract_eps_from_text(txt)\n",
    "        rows.append({\"filename\": fname, \"eps\": \"\" if eps is None else f\"{eps:.2f}\", \"note\": note or \"\"})\n",
    "    except Exception as e:\n",
    "        rows.append({\"filename\": fname, \"eps\": \"\", \"note\": f\"error: {e}\"})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Wrote: {os.path.abspath(OUTPUT_CSV)} | Rows: {len(df)}\")\n",
    "display(df.head(25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "279a5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f0b2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and dumped content from './Training_Filings/0000046080-20-000050.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000874766-20-000033.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000875320-20-000014.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001008654-20-000048.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001104659-20-052792.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001140361-20-010070.html '.\n",
      "Successfully processed and dumped content from './Training_Filings/0001157523-20-000597.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001165002-20-000083.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001171843-20-003035.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001193125-20-124288.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001193125-20-126089.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001193125-20-126683.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001373715-20-000098.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001423689-20-000040.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001564590-20-019431.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001564590-20-019442.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001576427-20-000032.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001620459-20-000067.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001722482-20-000089.html'.\n",
      "\n",
      "All content successfully dumped to 'output.txt'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def dump_html_to_txt(html_files, output_file):\n",
    "    \"\"\"\n",
    "    Reads content from a list of HTML files and dumps it into a single text file.\n",
    "\n",
    "    Args:\n",
    "        html_files (list): A list of strings, where each string is a path to an HTML file.\n",
    "        output_file (str): The path to the text file where content will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            for html_file in html_files:\n",
    "                if not os.path.exists(html_file):\n",
    "                    print(f\"Warning: File not found at '{html_file}'. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with open(html_file, 'r', encoding='utf-8') as infile:\n",
    "                        content = infile.read()\n",
    "\n",
    "                        outfile.write(f\"filename = {html_file}\\n\\n\")\n",
    "                        outfile.write(f'\"{content}\"\\n\\n\\n\\n')\n",
    "                        print(f\"Successfully processed and dumped content from '{html_file}'.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file '{html_file}': {e}\")\n",
    "\n",
    "        print(f\"\\nAll content successfully dumped to '{output_file}'.\")\n",
    "\n",
    "    except IOError as e:\n",
    "        print(f\"Error opening or writing to the output file '{output_file}': {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # List of HTML files you want to process.\n",
    "    # Add your HTML file paths to this list.\n",
    "    files_to_process = [\n",
    "        \"./Training_Filings/0000046080-20-000050.html\",\n",
    "        \"./Training_Filings/0000874766-20-000033.html\",\n",
    "        \"./Training_Filings/0000875320-20-000014.html\",\n",
    "        \"./Training_Filings/0001008654-20-000048.html\",\n",
    "        \"./Training_Filings/0001104659-20-052792.html\",\n",
    "        \"./Training_Filings/0001140361-20-010070.html \",\n",
    "        \"./Training_Filings/0001157523-20-000597.html\",\n",
    "        \"./Training_Filings/0001165002-20-000083.html\",\n",
    "        \"./Training_Filings/0001171843-20-003035.html\",\n",
    "        \"./Training_Filings/0001193125-20-124288.html\",\n",
    "        \"./Training_Filings/0001193125-20-126089.html\",\n",
    "        \"./Training_Filings/0001193125-20-126683.html\",\n",
    "        \"./Training_Filings/0001373715-20-000098.html\",\n",
    "        \"./Training_Filings/0001423689-20-000040.html\",\n",
    "        \"./Training_Filings/0001564590-20-019431.html\",\n",
    "        \"./Training_Filings/0001564590-20-019442.html\",\n",
    "        \"./Training_Filings/0001576427-20-000032.html\",\n",
    "        \"./Training_Filings/0001620459-20-000067.html\",\n",
    "        \"./Training_Filings/0001722482-20-000089.html\",\n",
    "    ]\n",
    "\n",
    "    # The name of the text file where you want to dump the content.\n",
    "    destination_file = 'output.txt'\n",
    "\n",
    "    dump_html_to_txt(files_to_process, destination_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c101f1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and dumped content from './Training_Filings/0000004977-20-000054.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000008947-20-000044.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000046080-20-000050.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000066570-20-000013.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000314808-20-000062.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000706129-20-000012.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000846617-20-000024.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000874766-20-000033.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000875320-20-000014.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000892537-20-000010.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000895419-20-000042.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000939057-20-000186.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0000950103-20-008424.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001008654-20-000048.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001104659-20-052683.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001104659-20-052792.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001104659-20-053353.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001104659-20-053534.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001104659-20-053563.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001140361-20-010070.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001141391-20-000089.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001157523-20-000597.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001157523-20-000599.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001157523-20-000600.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001165002-20-000083.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001171843-20-003035.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001193125-20-124288.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001193125-20-124568.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001193125-20-126089.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001193125-20-126683.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001289945-20-000036.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001299709-20-000078.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001323885-20-000027.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001373715-20-000098.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001423689-20-000040.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001436425-20-000011.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001538263-20-000014.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001564590-20-019396.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001564590-20-019421.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001564590-20-019431.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001564590-20-019442.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001564590-20-019726.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001564590-20-019755.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001564590-20-019760.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001576427-20-000032.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001620459-20-000067.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001678463-20-000062.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001691303-20-000019.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001720635-20-000018.html'.\n",
      "Successfully processed and dumped content from './Training_Filings/0001722482-20-000089.html'.\n",
      "\n",
      "All content successfully dumped to 'output_Files.txt'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def dump_html_to_txt(html_files, output_file):\n",
    "    \"\"\"\n",
    "    Reads content from a list of HTML files and dumps it into a single text file.\n",
    "\n",
    "    Args:\n",
    "        html_files (list): A list of strings, where each string is a path to an HTML file.\n",
    "        output_file (str): The path to the text file where content will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            for html_file in html_files:\n",
    "                if not os.path.exists(html_file):\n",
    "                    print(f\"Warning: File not found at '{html_file}'. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with open(html_file, 'r', encoding='utf-8') as infile:\n",
    "                        content = infile.read()\n",
    "\n",
    "                        outfile.write(f\"filename = {html_file}\\n\\n\")\n",
    "                        outfile.write(f'\"{content}\"\\n\\n\\n\\n')\n",
    "                        print(f\"Successfully processed and dumped content from '{html_file}'.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file '{html_file}': {e}\")\n",
    "\n",
    "        print(f\"\\nAll content successfully dumped to '{output_file}'.\")\n",
    "\n",
    "    except IOError as e:\n",
    "        print(f\"Error opening or writing to the output file '{output_file}': {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # The folder containing the HTML files you want to process.\n",
    "    # All files ending with .html in this folder will be processed.\n",
    "    source_folder = './Training_Filings/'\n",
    "\n",
    "    # The name of the text file where you want to dump the content.\n",
    "    destination_file = 'output_Files.txt'\n",
    "\n",
    "    # Check if the source folder exists\n",
    "    if not os.path.isdir(source_folder):\n",
    "        print(f\"Error: Source folder '{source_folder}' not found.\")\n",
    "        print(\"Please create the folder and place your HTML files inside it.\")\n",
    "    else:\n",
    "        # Find all files in the folder that end with .html\n",
    "        files_to_process = [\n",
    "            os.path.join(source_folder, f)\n",
    "            for f in os.listdir(source_folder)\n",
    "            if f.lower().endswith('.html')\n",
    "        ]\n",
    "\n",
    "        if not files_to_process:\n",
    "            print(f\"No HTML files found in '{source_folder}'.\")\n",
    "        else:\n",
    "            dump_html_to_txt(files_to_process, destination_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e87bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
